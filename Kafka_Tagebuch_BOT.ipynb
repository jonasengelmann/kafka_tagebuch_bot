{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kafka_Tagebuch_BOT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCp134XBiLzS",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jonasengelmann/kafka_tagebuch_bot/blob/master/Kafka_Tagebuch_BOT.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzVM_nlJiWC1",
        "colab_type": "text"
      },
      "source": [
        "Note: You can skip training/ fine-tuning and use my pretrained model to generate, just jump ahead to chapter 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqIr5pKLhir-",
        "colab_type": "text"
      },
      "source": [
        "# 0 Check Prerequisites\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOWVJD5rSZ1J",
        "colab_type": "text"
      },
      "source": [
        "We need a GPU with at lot of GPU RAM, so either P4s, T4s or P100s.  K80s unfortunately will not work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ-4T2nT0hJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3ecOWSE8OYw",
        "colab_type": "text"
      },
      "source": [
        "#1 Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyXn8_W95uYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/gooofy/transformer-lm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfAJJasFeVbR",
        "colab_type": "text"
      },
      "source": [
        "Update to latest tested revision to ensure functionality in the future:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxSijdK7hJkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd transformer-lm\n",
        "!git checkout eded3a7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah3LRUj1tu9t",
        "colab_type": "text"
      },
      "source": [
        "Install transformer-lm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHYZptjVtvzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -r requirements.txt\n",
        "!python setup.py develop\n",
        "!pip install json_log_plots"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmYBWwVwelCp",
        "colab_type": "text"
      },
      "source": [
        "Download Zamia's pretrained German model and unpack it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnjprOXfnr9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://goofy.zamia.org/zamia-speech/brain/gpt2-german-345M-r20191119.tar.xz -P ../model\n",
        "!tar xf ../model/gpt2-german-345M-r20191119.tar.xz -C ../model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt22tGYaAMmO",
        "colab_type": "text"
      },
      "source": [
        "The model's paramter *seen_tokens* has to be reset to allow finetuning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gcL47ALALsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "model_path = Path('../model') / 'de345-root' / 'model.pt'\n",
        "state = torch.load(model_path)\n",
        "state['seen_tokens'] = 0\n",
        "torch.save(state, model_path)\n",
        "del state\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmJQVC71vD4y",
        "colab_type": "text"
      },
      "source": [
        "#2 Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6M3ZCuh7RcK",
        "colab_type": "text"
      },
      "source": [
        " ### 2.1 On all of Kafka's work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPGGTe7Dcw9B",
        "colab_type": "text"
      },
      "source": [
        "Upload your dataset here, it should have the following folder structure:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVJ4dtj-aKXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# kafka_dataset/\n",
        "# |-----valid/\n",
        "#       |----valid.txt\n",
        "# |-----test/\n",
        "#       |----test.txt\n",
        "# |-----train/\n",
        "#       |----train.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv_p5clc1-2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR5gCWAh9V4s",
        "colab_type": "text"
      },
      "source": [
        "Unzip dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOn8acrvMgeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -a \"kafka_dataset.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywKakr3bt5NL",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing: dataset needs to be encoded with the sentencepiece model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9r_BsG3vGpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sp-encode kafka_dataset ../model/de345-root/sp.model kafka_dataset/encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOHNd_f4dL6-",
        "colab_type": "text"
      },
      "source": [
        "To avoid an error, we have to rename the sentencepiece model since it will be copied by transformer-lm into the model folder as *sp.model*, which however already exists:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9r7m-3yoKZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv ../model/de345-root/sp.model ../model/de345-root/sp_old.model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXpk_0FbuApq",
        "colab_type": "text"
      },
      "source": [
        "Finetuning: Most parameters are already predetermined by Zamia's German model, we can only change *batch-size* and *epochs*:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IILADlJUH0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gpt-2 \\\n",
        "    ../model/de345-root \\\n",
        "    kafka_dataset/encoded/ \\\n",
        "    ../model/de345-root/sp_old.model \\\n",
        "    --batch-size 1 \\\n",
        "    --g-accum-gradients 2 \\\n",
        "    --n-ctx 1024 \\\n",
        "    --n-embed 1024 \\\n",
        "    --n-hidden 1024 \\\n",
        "    --n-head 16 \\\n",
        "    --n-layer 24 \\\n",
        "    --epochs 4 \\\n",
        "    --lr=2e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqX0j5iFh9Xj",
        "colab_type": "text"
      },
      "source": [
        "Plot loss over steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0gHFcFwZ3tS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json_log_plots\n",
        "json_log_plots.plot(\"../model/de345-root\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmVUWr7hbC2D",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Fine-Tuning II: On Kafka's diaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4StgI_afaNO",
        "colab_type": "text"
      },
      "source": [
        "Upload your dataset here, it should have the following folder structure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE1rtiy1fe4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# kafka_diaries_dataset/\n",
        "# |-----valid/\n",
        "#       |----valid.txt\n",
        "# |-----test/\n",
        "#       |----test.txt\n",
        "# |-----train/\n",
        "#       |----train.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPMkVGQApZnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DPRRB9fpjyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -a \"kafka_diaries_dataset.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ4Cz-y8JbXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sp-encode kafka_diaries_dataset ../model/de345-root/sp.model kafka_diaries_dataset/encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwN4PNOEJgpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv ../model/de345-root/sp.model ../model/de345-root/sp_old2.model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImDKrT5-aHch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm ../model/de345-root/json-log-plots.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gG_2Mj9JV6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "model_path = Path('../model') / 'de345-root' / 'model.pt'\n",
        "state = torch.load(model_path)\n",
        "state['seen_tokens'] = 0\n",
        "torch.save(state, model_path)\n",
        "del state\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe7JbwzwJTjj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gpt-2 \\\n",
        "    ../model/de345-root \\\n",
        "    kafka_diaries_dataset/encoded/ \\\n",
        "    ../model/de345-root/sp_old2.model \\\n",
        "    --batch-size 1 \\\n",
        "    --g-accum-gradients 2 \\\n",
        "    --n-ctx 1024 \\\n",
        "    --n-embed 1024 \\\n",
        "    --n-hidden 1024 \\\n",
        "    --n-head 16 \\\n",
        "    --n-layer 24 \\\n",
        "    --epochs 10 \\\n",
        "    --lr=1.5e-5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlNNuPhN_Vo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json_log_plots\n",
        "json_log_plots.plot(\"../model/de345-root\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lZCFH-wibqA",
        "colab_type": "text"
      },
      "source": [
        "Quick test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRybn9AGiVv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gpt-2-gen ../model/de345-root \"10. August. \""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA9xN-zW7FEK",
        "colab_type": "text"
      },
      "source": [
        "#3 Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvBsl68lk9hE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N8Qnmoo7mpt",
        "colab_type": "text"
      },
      "source": [
        "###A) Use model trained in chapter 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH_cOuWyJq5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv model/de345-root model/kafka_diary_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rcH0a2M7uvF",
        "colab_type": "text"
      },
      "source": [
        "##B) Use my pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op-eYAxd77dF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1qBpljKf0odtwZkG9V8hivOXhVG1g190C' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1qBpljKf0odtwZkG9V8hivOXhVG1g190C\" -O model/kafka_diary_model.tar.xz && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRQ4OHfh7t-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar xf model/kafka_diary_model.tar.xz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhDAVD2ubOnW",
        "colab_type": "text"
      },
      "source": [
        "We will use another fork, which has more generation parameters available:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS4eT7Y2puqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/lopuhin/transformer-lm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZC9mBdOp1a_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd transformer-lm\n",
        "!git checkout c369833 #last tested revision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0lnhIBfqHFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -r requirements.txt\n",
        "!python setup.py develop\n",
        "!pip install babel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY7zs_rpeyNP",
        "colab_type": "text"
      },
      "source": [
        "## Generating a single entry:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf1I5Ji6bYzh",
        "colab_type": "text"
      },
      "source": [
        "Some adjustments are needed to allow for batch generation and omitting *unk* tokens from the output. The code was also edited so that it continues generating tokens until it reaches the end of a sentnece. However, this can sometimes cause it to be stuck in a loop, especially with low values for top_k or temperature. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JuZduMvqeH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Third party imports:\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Local application imports:\n",
        "from lm import inference\n",
        "from lm.common import END_OF_LINE, END_OF_TEXT\n",
        "\n",
        "class ModelWrapperModified(inference.ModelWrapper):\n",
        "    END_OF_LINE = END_OF_LINE\n",
        "    END_OF_TEXT = END_OF_TEXT\n",
        "    \n",
        "    def __init__(self, model, sp_model, params):\n",
        "        super().__init__(model, sp_model, params)\n",
        "\n",
        "    def generate_tokens(self, tokens_prefix, tokens_to_generate, top_k, top_p=1, temperature=1):\n",
        "        tokens = list(tokens_prefix)\n",
        "        output_tokens = []\n",
        "        past = None\n",
        "\n",
        "        i = 0\n",
        "        while True:\n",
        "\n",
        "            if top_p <= 0.0:\n",
        "                # generate TOP_K potential next tokens\n",
        "                ntk, presents = self._get_next_top_k(tokens, top_k, past=past)\n",
        "\n",
        "                # Remove unk tokens:\n",
        "                ntk = [\n",
        "                        x\n",
        "                        for x in ntk\n",
        "                        if x[1] != '<unk>'\n",
        "                ]\n",
        "\n",
        "                # convert log probs to real probs\n",
        "                logprobs = np.array(list(map(lambda a: a[0], ntk)))\n",
        "                logprobs /= temperature\n",
        "                probs = np.exp(logprobs) / np.exp(logprobs).sum()\n",
        "\n",
        "                # pick next token randomly according to probs distribution\n",
        "                next_token_n = np.random.choice(len(ntk), p=probs)\n",
        "                next_token = ntk[next_token_n][1]\n",
        "            else:\n",
        "                filtered_logprobs, presents = self._get_next_top_p_nucleus(\n",
        "                    tokens, top_p, past=past)\n",
        "          \n",
        "                filtered_logprobs /= temperature\n",
        "                next_token_n = torch.multinomial(torch.nn.functional.softmax(\n",
        "                    filtered_logprobs, dim=-1), num_samples=4)\n",
        "                \n",
        "                for x in next_token_n:\n",
        "                  next_token = self.id_to_token(x)\n",
        "                  if next_token != '<unk>':\n",
        "                    break\n",
        "\n",
        "            if past is None:\n",
        "                past = presents\n",
        "            else:\n",
        "                past = torch.cat([past, presents], dim=-2)\n",
        "\n",
        "            tokens = [next_token]\n",
        "            output_tokens.append(next_token)\n",
        "\n",
        "            i += 1\n",
        "\n",
        "            # Terminate generation when end of sentence is reached:\n",
        "            if i >= tokens_to_generate and next_token in ['!', '.', '?']:\n",
        "                break\n",
        "            elif next_token == '<endoftext>':\n",
        "                break\n",
        "\n",
        "        return output_tokens\n",
        "\n",
        "class BatchTextGeneration:\n",
        "    def __init__(self, model_path):\n",
        "        print(\"loading model from %s\" % model_path)\n",
        "        self.mw = ModelWrapperModified.load(Path(model_path))\n",
        "\n",
        "    def generate_text(self, prefix, tokens_to_generate, top_k, top_p=0, temperature=1):\n",
        "        tokens = self.mw.tokenize(prefix)\n",
        "        tokens_gen = self.mw.generate_tokens(tokens, tokens_to_generate, top_k, top_p, temperature)\n",
        "        return self.mw.sp_model.DecodePieces(tokens_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuDrk966qicd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BatchTextGeneration('../model/kafka_diary_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2gbEO2EqpAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.generate_text(\"19. August. \", tokens_to_generate=30, top_k=30, top_p=0, temperature=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqzwcrfqetRU",
        "colab_type": "text"
      },
      "source": [
        "## Generating entries for a whole year:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zdn6xMY0enak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens_to_generate = 30\n",
        "top_k = 30\n",
        "top_p = 0\n",
        "temperature = 1\n",
        "\n",
        "## iterate over dates:\n",
        "from datetime import timedelta, date, time\n",
        "from babel.dates import format_datetime\n",
        "\n",
        "def daterange(start_date, end_date):\n",
        "    for n in range(int((end_date - start_date).days)):\n",
        "        yield start_date + timedelta(n)\n",
        "\n",
        "start_date = date(2020, 1, 1)\n",
        "end_date = date(2021, 1, 1)\n",
        "for single_date in daterange(start_date, end_date):\n",
        "    promt = format_datetime(single_date, \"d. MMMM. \", locale='de_DE')\n",
        "    print(promt + model.generate_text(\n",
        "        promt,\n",
        "        tokens_to_generate=tokens_to_generate,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        temperature=temperature\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}